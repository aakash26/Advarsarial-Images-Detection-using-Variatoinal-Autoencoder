{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    " \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import json \n",
    "import time \n",
    "import pickle \n",
    "import sys \n",
    "import csv \n",
    "import os \n",
    "import os.path as osp \n",
    "import shutil \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    " \n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots \n",
    "plt.rcParams['image.interpolation'] = 'nearest' \n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    " \n",
    "# for auto-reloading external modules \n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Some suggestions of our libraries that might be helpful for this project\n",
    "from collections import Counter          # an even easier way to count\n",
    "from multiprocessing import Pool         # for multiprocessing\n",
    "from tqdm import tqdm                    # fancy progress bars\n",
    "import time as timer\n",
    "\n",
    "# Load other libraries here.\n",
    "# Keep it minimal! We should be easily able to reproduce your code.\n",
    "# We only support sklearn and pytorch.\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import foolbox\n",
    "import wget\n",
    "# We preload pytorch as an example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "from art.attacks import *\n",
    "from art.classifiers import PyTorchClassifier\n",
    "from art.utils import load_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LeNet Model definition\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "#Location for pre-trained model: \n",
    "#url=\"https://drive.google.com/drive/folders/1fn83DF14tWmit0RTKWRhPq5uVXt73e0h?usp=sharing\"\n",
    "#Saved in  \"Models/lenet_mnist_model.pth\"\n",
    "\n",
    "    \n",
    "pretrained_model = \"Models/lenet_mnist_model.pth\"\n",
    "use_cuda=False\n",
    "    \n",
    "    \n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "#device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Initialize the network\n",
    "model = LeNet5().to(device)\n",
    "\n",
    "# Load the pretrained model\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section A: Preparing Foolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foolbox: data conversion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Foolbox_x_y_generator(dataset):\n",
    "    '''\n",
    "    will convert a PyTorch dataset to the format required by Foolbox for data processing \n",
    "    \n",
    "    '''\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "    images = next(iter(data_loader))[0].numpy()\n",
    "    \n",
    "    #Todo: delete this \n",
    "    #labels = np.full((len(dataset),10),0.0,dtype=\"float32\")\n",
    "    \n",
    "    labels = dataset.targets.numpy()\n",
    "            \n",
    "    return images, labels \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foolbox: convert training dataset into foolbox nparray format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Test dataset and dataloader declaration\n",
    "train_dataset = datasets.MNIST('data/MNIST/', train=True, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            ]))\n",
    "\n",
    "x_train_foolbox, y_train_foolbox = Foolbox_x_y_generator(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foolbox: attack function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foolbox_attack_demo(attack_title, attack_type,images,labels):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    adversarials = attack_type(images, labels, unpack=False)\n",
    "    \n",
    "    distances = np.asarray([a.distance.value for a in adversarials])\n",
    "    failed_attacks = sum(adv.distance.value == np.inf for adv in adversarials)\n",
    "    accuracy_on_adversarial = (failed_attacks/len(adversarials))*100 \n",
    "    \n",
    "    time_taken = round((time.time() - start_time),2)\n",
    "    \n",
    "    print(\"Foolbox model: Accuracy on pertubed images: {} %, Time taken:{} seconds\".format(accuracy_on_adversarial,time_taken))\n",
    "\n",
    "    \n",
    "    arr = []\n",
    "    for each in range(len(adversarials)):\n",
    "        \n",
    "        #return predicted class if it exists else, return the original class \n",
    "        try:\n",
    "            predicted_class = adversarials[each].output.argmax()\n",
    "        except AttributeError:\n",
    "            predicted_class =  adversarials[each].original_class\n",
    "            \n",
    "        #return preturbed image if it exists else, return the original image itself \n",
    "        try:\n",
    "            preturbed_image = adversarials[each].perturbed.reshape(28,28)\n",
    "        except AttributeError:\n",
    "            preturbed_image = adversarials[each].unperturbed.reshape(28,28)\n",
    "\n",
    "\n",
    "            \n",
    "        arr.append((preturbed_image,\n",
    "                    adversarials[each].unperturbed.reshape(28,28),\n",
    "                    adversarials[each].original_class, \n",
    "                    predicted_class))\n",
    "\n",
    "    #the actual adversarial images will be saved locally in a pickle file \n",
    "    \n",
    "    timestamp=datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\")\n",
    "    file_name = \"data/Adversarial/foolboxadv_\" +attack_title+ \"_\"+timestamp + \".pickle\"\n",
    "    pickle_file = open(file_name,\"wb\")\n",
    "    pickle.dump(arr, pickle_file)\n",
    "    pickle_file.close()\n",
    "    print(\"Foolbox: Adversarials for \" +attack_title + \" saved as pickle file\")\n",
    "    \n",
    "\n",
    "    return [attack_title,accuracy_on_adversarial, time_taken]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section B: Preparing IBM ART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM ART: data conversion function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ART_x_y_generator(dataset):\n",
    "    '''\n",
    "    will convert a PyTorch dataset to the format required by IBM ART for data processing \n",
    "    \n",
    "    '''\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "    images = next(iter(data_loader))[0].numpy()\n",
    "    \n",
    "    labels = np.full((len(dataset),10),0.0,dtype=\"float32\")\n",
    "    \n",
    "    target_list = dataset.targets.numpy()\n",
    "    \n",
    "    for index, target in enumerate(target_list):\n",
    "        labels[index][target]=1\n",
    "        \n",
    "    return images, labels , target_list\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Test dataset and dataloader declaration\n",
    "train_dataset = datasets.MNIST('data/MNIST/', train=True, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            ]))\n",
    "\n",
    "x_train_IBM, y_train_IBM, y_train_oldformat_IBM = ART_x_y_generator(train_dataset)\n",
    "\n",
    "# MNIST Test dataset and dataloader declaration\n",
    "test_dataset = datasets.MNIST('data/MNIST/', train=False, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            ]))\n",
    "\n",
    "x_test_IBM, y_test_IBM, y_test_oldformat_IBM = ART_x_y_generator(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare classifer for IBM ART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "min_pixel_value=0.0\n",
    "max_pixel_value=1.0 \n",
    "\n",
    "classifier = PyTorchClassifier(model=model\n",
    "                               , clip_values=(min_pixel_value, max_pixel_value)\n",
    "                               , loss=criterion\n",
    "                               , optimizer=optimizer\n",
    "                               , input_shape=(1, 28, 28)\n",
    "                               , nb_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 98.87%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier.fit(x_train_IBM, y_train_IBM, batch_size=64, nb_epochs=3)\n",
    "\n",
    "predictions = classifier.predict(x_test_IBM)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test_IBM, axis=1)) / len(y_test_IBM)\n",
    "print('Accuracy on benign test examples: {}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM ART: attack function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ART_attack_demo(attack_title, attack_type,classifier,images,labels,labels_original_format):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    attack = attack_type\n",
    "    images_adv = attack.generate(x=images)\n",
    "    predictions = classifier.predict(images_adv)\n",
    "    \n",
    "    time_taken = round((time.time() - start_time),2)\n",
    "    accuracy_on_adversarial = (np.sum(np.argmax(predictions, axis=1) == np.argmax(labels, axis=1)) / len(labels))*100\n",
    "    \n",
    "    print(\"IBM ART: Accuracy on pertubed images: {} %, Time taken:{} seconds\".format(accuracy_on_adversarial,time_taken))\n",
    "    \n",
    "    #the actual adversarial images will be saved locally in a pickle file \n",
    "    \n",
    "    timestamp=datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\")\n",
    "    file_name = \"data/Adversarial/IBMART_Image_and_label_\" +attack_title+ \"_\"+timestamp + \".pickle\"\n",
    "    pickle_file = open(file_name,\"wb\")\n",
    "    adversarials = [images_adv, labels_original_format]\n",
    "    pickle.dump(adversarials, pickle_file)\n",
    "    pickle_file.close()\n",
    "    print(\"IBM ART: Adversarials for \" +attack_title + \" saved as pickle file\")\n",
    "\n",
    "    return [attack_title,accuracy_on_adversarial, time_taken]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section C: Attack comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 30000\n",
    "\n",
    "\n",
    "x_train_IBM = x_train_IBM[:number_of_samples]\n",
    "y_train_IBM = x_train_IBM[:number_of_samples]\n",
    "y_train_oldformat_IBM = y_train_oldformat_IBM [:number_of_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM ART: Accuracy on pertubed images: 0.0 %, Time taken:107.7 seconds\n",
      "IBM ART: Adversarials for FastGradientMethod saved as pickle file\n"
     ]
    }
   ],
   "source": [
    "art_attack_type = FastGradientMethod(classifier=classifier)\n",
    "art_adversarials = ART_attack_demo(\"FastGradientMethod\",art_attack_type,classifier,x_train_IBM,y_train_IBM,y_train_oldformat_IBM)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carlini Wagner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_attack_type = CarliniLInfMethod(classifier=classifier)\n",
    "art_adversarials = ART_attack_demo(\"CarliniLInfMethod\",art_attack_type,classifier,x_train_IBM,y_train_IBM,y_train_oldformat_IBM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DeepFool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_attack_type = DeepFool(classifier=classifier)\n",
    "art_adversarials = ART_attack_demo(\"DeepFool\",art_attack_type,classifier,x_train_IBM,y_train_IBM,y_train_oldformat_IBM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foolbox code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_foolbox = x_train_foolbox [:number_of_samples] \n",
    "y_train_foolbox = y_train_foolbox [:number_of_samples]\n",
    "\n",
    "\n",
    "foolboxmodel = foolbox.models.PyTorchModel(model, bounds=(0, 1), num_classes=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FGSM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foolbox_attack_type = foolbox.attacks.FGSM(foolboxmodel)\n",
    "foolbox_adversarials = foolbox_attack_demo(\"FGSM\", foolbox_attack_type,x_train_foolbox,y_train_foolbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carlini Wagner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DeepFool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Code to read data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM - Pickle and Shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def pickleretrieve(file_name):\n",
    "    variable  = pickle.load( open( file_name, \"rb\" ) )\n",
    "    return variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = 'data/Adversarial/IBMART_Image_and_label_FastGradientMethod_25_01_2020__10_15_58.pickle'\n",
    "IBM_FGSM_Adversarials = pickleretrieve(sample_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 1, 28, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IBM_adversarial_images,IBM_original_labels = iter(IBM_FGSM_Adversarials) \n",
    "IBM_adversarial_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(IBM_original_labels[10])\n",
    "import matplotlib.pyplot as plt \n",
    "img = IBM_adversarial_images[10].reshape((28,28))\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
